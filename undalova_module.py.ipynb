{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757c4c54-2dbb-46ec-a1c5-2635ce21d507",
   "metadata": {},
   "source": [
    "Margarita Undalova\n",
    "\n",
    "01/1365289\n",
    "\n",
    "This is the file that contains functions to perform all the necessary tasks. \n",
    "This module contains functions for analyzing text data. Includes loading documents, computing statistics, term frequencies, type-token ratios, n-grams, and collocations.\n",
    "\n",
    "Modular functions:\n",
    "\n",
    "- load_documents: loads text documents from a specified directory.\n",
    "- compute_statistics: computes basic statistics such as line counts and token counts.\n",
    "- term_frequency: computes term frequencies for a given text.\n",
    "- compute_term_frequencies: computes term frequencies for all documents and global term frequencies.\n",
    "- type_token_ratio: computes type-token ratios for a given text.\n",
    "- compute_ttr_for_documents: computes type-token ratios for all documents and the entire corpus.\n",
    "- generate_ngrams: generates n-grams for a given text.\n",
    "- compute_ngrams_for_documents: calculates n-grams for all documents and the entire corpus.\n",
    "- generate_concordance: generates a concordance for a given word in the text.\n",
    "- find_collocations: finds collocations (bigrams or trigrams) in the text.\n",
    "- generate_unique_words: generates unique words from the text.\n",
    "- save_unique_words_to_file: saves unique words to a file.\n",
    "\n",
    "Note: The `nltk` and `matplotlib` libraries are used to work with text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56045a22-70a6-4cf8-92b0-183d99759a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /users/margarita.undalova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def load_documents(directory):\n",
    "    \"\"\"\n",
    "    Loads text documents from the specified directory.\n",
    "    Arguments: \n",
    "    directory (str): Path to the directory containing the text files.\n",
    "    Returns: \n",
    "    dict: A dictionary where the keys are the file names and the values are the file contents.\n",
    "    \"\"\"\n",
    "    documents = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                documents[filename] = text\n",
    "    return documents\n",
    "\n",
    "def compute_statistics(documents):\n",
    "    \"\"\"\n",
    "    Computes basic statistics including number of lines and tokens.\n",
    "    Arguments: \n",
    "    documents (dict): A dictionary of documents, where keys are file names and values are text.\n",
    "    Returns:\n",
    "    dict: A dictionary of statistics, including the total number of documents, lines, tokens, and statistics for each document.\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'total_documents': len(documents),\n",
    "        'total_lines': 0,\n",
    "        'total_tokens': 0,\n",
    "        'document_stats': {}\n",
    "    }\n",
    "    for filename, text in documents.items():\n",
    "        lines = text.splitlines()\n",
    "        tokens = word_tokenize(text)\n",
    "        doc_stats = {\n",
    "            'num_lines': len(lines),\n",
    "            'num_tokens': len(tokens)\n",
    "        }\n",
    "        stats['total_lines'] += doc_stats['num_lines']\n",
    "        stats['total_tokens'] += doc_stats['num_tokens']\n",
    "        stats['document_stats'][filename] = doc_stats\n",
    "    return stats\n",
    "\n",
    "def term_frequency(text):\n",
    "    \"\"\"\n",
    "    Computes term frequency for a given text.\n",
    "    Arguments: text (str): The text to analyze.\n",
    "    Returns: \n",
    "    collections.Counter: A dictionary where the keys are words, and the values are their frequencies.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    return Counter(words)\n",
    "\n",
    "def compute_term_frequencies(documents):\n",
    "    \"\"\"\n",
    "    Computes term frequencies for all documents and global term frequencies.\n",
    "    Arguments: \n",
    "    documents (dict): A dictionary of documents, where keys are file names and values are text.\n",
    "    Returns: \n",
    "    tuple: A tuple containing two elements:\n",
    "    - dict: A dictionary of term frequencies for each document.\n",
    "    - collections.Counter: Global term frequencies for the entire corpus.\n",
    "\n",
    "    \"\"\"\n",
    "    global_term_freq = Counter()\n",
    "    document_term_frequencies = {}\n",
    "    for filename, text in documents.items():\n",
    "        term_freq = term_frequency(text)\n",
    "        document_term_frequencies[filename] = term_freq\n",
    "        global_term_freq.update(term_freq)\n",
    "    return document_term_frequencies, global_term_freq\n",
    "\n",
    "def type_token_ratio(text):\n",
    "    \"\"\"\n",
    "    Computes the type-token ratio for a given text.\n",
    "    Arguments: \n",
    "    text (str): Text to analyze.\n",
    "    Returns: \n",
    "    float: Ratio of unique words to total words.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    num_tokens = len(words)\n",
    "    num_types = len(set(words))\n",
    "    return num_types / num_tokens if num_tokens > 0 else 0\n",
    "\n",
    "def compute_ttr_for_documents(documents):\n",
    "    \"\"\"\n",
    "    Computes the type-token ratio for all documents and for the entire corpus.\n",
    "    Arguments: \n",
    "    documents (dict): A dictionary of documents, where keys are filenames and values are text.\n",
    "    Returns:\n",
    "    tuple: A tuple containing two elements:\n",
    "    - dict: A dictionary of type-to-token mappings for each document.\n",
    "    - float: The type-to-token mappings for the entire corpus.\n",
    "    \"\"\"\n",
    "    document_ttrs = {}\n",
    "    combined_words = []\n",
    "    for filename, text in documents.items():\n",
    "        ttr = type_token_ratio(text)\n",
    "        document_ttrs[filename] = ttr\n",
    "        combined_words.extend(word_tokenize(text.lower()))\n",
    "    global_ttr = type_token_ratio(' '.join(combined_words))\n",
    "    return document_ttrs, global_ttr\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    \"\"\"\n",
    "    Generates n-grams for a given text.\n",
    "    Arguments:\n",
    "    text (str): Text to analyze.\n",
    "    n (int): Size of n-grams (e.g. 1 for unigrams, 2 for bigrams, etc.).\n",
    "    Returns: collections.\n",
    "    Counter: A dictionary where keys are n-grams and values are their frequencies.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    return Counter(n_grams)\n",
    "\n",
    "def compute_ngrams_for_documents(documents):\n",
    "    \"\"\"\n",
    "    Computes n-grams for all documents and for the entire corpus.\n",
    "    Computes n-grams for all documents and the entire corpus.\n",
    "    Arguments: \n",
    "    documents (dict): A dictionary of documents, where keys are filenames and values are text.\n",
    "    Returns:\n",
    "    tuple: A tuple containing two elements:\n",
    "    - dict: A dictionary of n-grams (unigrams, bigrams, and trigrams) for each document.\n",
    "    - dict: A dictionary of n-grams (unigrams, bigrams, and trigrams) for the entire corpus.\n",
    "    \"\"\"\n",
    "    document_ngrams = {}\n",
    "    combined_tokens = []\n",
    "    for filename, text in documents.items():\n",
    "        unigrams = generate_ngrams(text, 1)\n",
    "        bigrams = generate_ngrams(text, 2)\n",
    "        trigrams = generate_ngrams(text, 3)\n",
    "        document_ngrams[filename] = {\n",
    "            'unigrams': unigrams,\n",
    "            'bigrams': bigrams,\n",
    "            'trigrams': trigrams\n",
    "        }\n",
    "        combined_tokens.extend(word_tokenize(text.lower()))\n",
    "    combined_text = ' '.join(combined_tokens)\n",
    "    corpus_ngrams = {\n",
    "        'unigrams': generate_ngrams(combined_text, 1),\n",
    "        'bigrams': generate_ngrams(combined_text, 2),\n",
    "        'trigrams': generate_ngrams(combined_text, 3)\n",
    "    }\n",
    "    return document_ngrams, corpus_ngrams\n",
    "\n",
    "def generate_concordance(text, word, window=5):\n",
    "    \"\"\"\n",
    "    Generates concordance for a given word in a text.\n",
    "    Arguments: \n",
    "    text (str): Text to analyze.\n",
    "    word (str): Word to search for in text.\n",
    "    window (int): Window size for context.\n",
    "    Returns:\n",
    "    list: List of tuples, each containing a left context, a search word, and a right context.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    text_obj = nltk.Text(tokens)\n",
    "    concordance_list = []\n",
    "    for index in range(len(tokens)):\n",
    "        if tokens[index] == word.lower():\n",
    "            left_context = tokens[max(index - window, 0):index]\n",
    "            right_context = tokens[index + 1:min(index + window + 1, len(tokens))]\n",
    "            concordance_list.append((' '.join(left_context), word, ' '.join(right_context)))\n",
    "    return concordance_list\n",
    "\n",
    "def find_collocations(text, n):\n",
    "    \"\"\"\n",
    "    Finds collocations (bigrams or trigrams) in a text.\n",
    "    Arguments:\n",
    "    text (str): Text to analyze.\n",
    "    n (int): Size of collocations (2 for bigrams, 3 for trigrams).\n",
    "    Returns:\n",
    "    list: List of tuples, where each tuple contains an n-gram and its score.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    if n == 2:\n",
    "        finder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "        scored = finder.score_ngrams(nltk.metrics.BigramAssocMeasures.likelihood_ratio)\n",
    "    elif n == 3:\n",
    "        finder = nltk.collocations.TrigramCollocationFinder.from_words(tokens)\n",
    "        scored = finder.score_ngrams(nltk.metrics.TrigramAssocMeasures.likelihood_ratio)\n",
    "    else:\n",
    "        raise ValueError(\"Only bigrams and trigrams are supported\")\n",
    "    return scored\n",
    "\n",
    "def generate_unique_words(text):\n",
    "    \"\"\"\n",
    "    Generates unique words from the text.\n",
    "    Arguments:\n",
    "    text (str): Text to analyze.\n",
    "    Returns:\n",
    "    set: Set of unique words in the text.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    return set(tokens)\n",
    "\n",
    "def save_unique_words_to_file(filename, unique_words):\n",
    "    \"\"\"\n",
    "    Save unique words to a file.\n",
    "    Arguments:\n",
    "    filename (str): Path to file to save unique words.\n",
    "    unique_words (set): Set of unique words to save.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        for word in sorted(unique_words):\n",
    "            file.write(f\"{word}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604bd11f-2930-4f25-9475-783401c5420f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
